# â˜ï¸ Snowflake + Azure + Databricks Community Edition â€“ Step-by-Step Guide

> ðŸŽ¯ **Goal:**  
Set up Snowflake and Azure Data Lake, connect them with the *free Databricks Community Edition*, move data from Azure â†’ Databricks â†’ Snowflake, and query Snowflake using the Snowflake Connector and Snowpark API.

---

## ðŸ§© 1. Prerequisites

| Tool | Requirement | Notes |
|------|--------------|-------|
| **Snowflake** | Free 30-day trial | Choose Azure as cloud provider |
| **Databricks** | [Community Edition](https://community.cloud.databricks.com) | 100% free; hosted by Databricks |
| **Azure** | Free Subscription | Used for Azure Blob / Data Lake Storage |
| **Python 3.x** | Installed locally | Optional â€“ for Snowpark script testing |

---

## âš™ï¸ 2. Step 1 â€” Create & Configure Snowflake Trial

1. Visit: [https://signup.snowflake.com](https://signup.snowflake.com)
2. Choose:
   - **Cloud provider:** Microsoft Azure  
   - **Region:** your nearest Azure region (e.g. *East US 2*)
3. Log in to **Snowsight (UI)** â†’ Youâ€™ll get a URL like:  
   `https://xy12345.east-us-2.azure.snowflakecomputing.com`
4. Note down:
   - `Account` (e.g., `xy12345`)
   - `Username`
   - `Password`
   - `Warehouse`: `COMPUTE_WH`
   - `Database`: `MY_PRACTICE_DB`
   - `Schema`: `MY_SCHEMA`

âœ… **Test your setup**
```sql
CREATE OR REPLACE TABLE SALES (
  ID INT, CUSTOMER STRING, REGION STRING, AMOUNT NUMBER(10,2)
);
INSERT INTO SALES VALUES (1, 'John', 'West', 5000);
SELECT * FROM SALES;
```

---

## ðŸ§± 3. Step 2 â€” Set Up Azure Storage (ADLS / Blob)

1. In Azure Portal â†’ **Storage Accounts â†’ + Create**
2. Resource group â†’ choose any
3. Storage account name â†’ `hexadatalakestore`
4. Region â†’ same as Snowflake region  
5. After creation â†’ Go to **Access keys** and copy:
   - **Storage account name**
   - **Access key**
6. Under **Containers**, create:
   - Container name â†’ `salesdata`
   - Upload CSV file â†’ `sales_2025_10_13.csv`
![alt text](image.png)
âœ… Example CSV:
```csv
ID,CUSTOMER,REGION,AMOUNT
1,John,West,5000
2,Priya,East,3200
3,David,North,4100
4,Meena,South,2800
```
![alt text](image-1.png)
---

## ðŸ’» 4. Step 3 â€” Launch Databricks Community Edition

1. Go to [https://community.cloud.databricks.com](https://community.cloud.databricks.com)
2. Sign in with any email.
3. Click **Compute â†’ Create Cluster**
   - Name: `CommunityCluster`
   - Runtime: `11.x (includes Apache Spark 3.x, Scala 2.12)`
4. Wait until it shows **Running (green dot)** âœ…

---

## ðŸ”— 5. Step 4 â€” Access Azure Blob Storage from Databricks CE

> âš ï¸ *Community Edition doesnâ€™t allow dbutils.fs.mount(), but you can access via wasbs://*

### Configure Access Key
```python
import pandas as pd

url = (
    "https://hexadatalakestore.blob.core.windows.net/"
    "salesdata/sales_2025_10_13.csv"
    "?sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupiytfx"
    "&se=2025-10-14T18:52:49Z&st=2025-10-14T10:37:49Z"
    "&spr=https&sig=8hWQU%2BEtYbKnHXwp%2Bdt1wOAFrmBtIqzZRy41BWRbCQE%3D"
)

df = pd.read_csv(url)
print(df.head())

```

![alt text](image-2.png)

âœ… Youâ€™ll see your Azure file loaded into Databricks.

---

## â„ï¸ 6. Step 5 â€” Connect Databricks to Snowflake

```python
%pip install snowflake-connector-python[pandas]

```

```python
 %restart_python
```
### Define connection options
```python
import snowflake.connector
from snowflake.connector.pandas_tools import write_pandas
import pandas as pd

url = (
    "https://hexadatalakestore.blob.core.windows.net/"
    "salesdata/sales_2025_10_13.csv"
    "?sv=2024-11-04&ss=bfqt&srt=sco&sp=rwdlacupiytfx"
    "&se=2025-10-14T18:52:49Z&st=2025-10-14T10:37:49Z"
    "&spr=https&sig=8hWQU%2BEtYbKnHXwp%2Bdt1wOAFrmBtIqzZRy41BWRbCQE%3D"
)

df = pd.read_csv(url, sep=",", encoding="utf-8-sig")
#df.columns = ["ID", "CUSTOMER", "REGION", "AMOUNT"]
#df["ID"] = df["ID"].astype(int)
#df["AMOUNT"] = df["AMOUNT"].astype(float)

conn = snowflake.connector.connect(
    user="GEETHA",
    password="MySnowflakecred1",
    account="YGVSEGU-JE02852",
    warehouse="COMPUTE_WH",
    database="MY_PRACTICE_DB",
    schema="MY_SCHEMA"
)

success, nchunks, nrows, _ = write_pandas(conn, df, "SALES_FROM_AZURE")
print(f"âœ… Uploaded {nrows} rows successfully!")
conn.close()

```

![alt text](image-3.png)

## ðŸ‘¥ 7. Step 6 â€” Create Role and Privileges (Optional)

In Snowflake:
```sql
CREATE ROLE DATABRICKS_ROLE;
GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE DATABRICKS_ROLE;
GRANT USAGE ON DATABASE TRAINING_DB TO ROLE DATABRICKS_ROLE;
GRANT USAGE ON SCHEMA PUBLIC TO ROLE DATABRICKS_ROLE;
GRANT SELECT, INSERT ON ALL TABLES IN SCHEMA PUBLIC TO ROLE DATABRICKS_ROLE;
GRANT ROLE DATABRICKS_ROLE TO USER TRAINER_USER;
```

âœ… Use this role in Databricks under `sfRole`.

---

## ðŸª 8. Step 7 â€” Explore Snowflake Marketplace

In Snowsight â†’ **Data â†’ Marketplace â†’ Search â€œCOVID-19â€ or â€œWeather Sourceâ€**  
â†’ Click **Get Data â†’ Add to account**

Then query:
```sql
USE DATABASE COVID19_EPIDEMIOLOGICAL_DATA;
SELECT * FROM EPIDEMIOLOGY LIMIT 10;
```

---

## ðŸ§  9. Step 8 â€” Query Snowflake Using Snowpark

### Install Snowpark
```python
%pip install snowflake-snowpark-python
```

```python
%restart_python
```
### Initialize Session
```python
from snowflake.snowpark import Session
from snowflake.snowpark.functions import col

connection_params = {
    "user":"GEETHA",
    "password":"MySnowflakecred1",
    "account":"YGVSEGU-JE02852",
    "warehouse":"COMPUTE_WH",
    "database":"MY_PRACTICE_DB",
    "schema":"MY_SCHEMA"
}

session = Session.builder.configs(connection_params).create()
```

### Query
```python
df_snow = session.table("SALES_FROM_AZURE")
df_snow.filter(col("AMOUNT") > 3000).show()
```

âœ… Output:
```
+----+---------+--------+--------+
| ID | CUSTOMER| REGION | AMOUNT |
+----+---------+--------+--------+
| 1  | John    | West   | 5000.0 |
| 2  | Priya   | East   | 3200.0 |
+----+---------+--------+--------+
```

---

## ðŸ“Š 10. End-to-End Architecture
```
Azure Blob (ADLS)
    â†“
Databricks Community Edition (Spark / Snowpark)
    â†“
Snowflake (Warehouse + Schema + Table)
```

---

## âœ… Outcome Summary

| Task | Status |
|------|--------|
| Create Snowflake Trial | âœ… |
| Create Azure Blob Storage | âœ… |
| Read Azure file in Databricks CE | âœ… |
| Write data into Snowflake | âœ… |
| Query in Snowflake | âœ… |
| Run Snowpark Script | âœ… |

---

## ðŸ§¾ Trainer Notes
- Databricks CE runs on Databricksâ€™ infra (not Azure VMs), but connects fine to Azure and Snowflake.
- `dbutils.fs.mount` wonâ€™t work â€” use direct `wasbs://` access.
- Use `ACCOUNTADMIN` role for learning to avoid permission issues.
- Drop tables post-lab to save Snowflake credits:
  ```sql
  DROP TABLE IF EXISTS SALES_FROM_AZURE;
  ```

---

**Author:** *Prepared by Snowflake & Databricks Specialist Trainer (GPTâ€‘5)*  
**Module:** Databricks CE + Azure + Snowflake Integration  
**Audience:** Beginners & Data Engineering Learners
