
# üßä Snowflake Assignment ‚Äî Load S3 Data into a Table (Trainer Notes)

These step-by-step notes show how to load customer data from **S3** into a Snowflake table using an **external stage**, then verify how many rows were loaded.

---

## ‚úÖ Prerequisites
- A running **warehouse**.
- Role with **USAGE** on the database/schema and **INSERT** on the table.
- The S3 location appears to be **public**: **`s3://snowflake-assignments-mc/loadingdata/`**  
  (If it isn‚Äôt public, you‚Äôll need a **storage integration** or access keys.)

---

## 1) Create the **database** (if needed) and use it
```sql
-- Create the database (skip if already created)
CREATE DATABASE IF NOT EXISTS EXERCISE_DB;

-- Set context
USE DATABASE EXERCISE_DB;
USE SCHEMA PUBLIC;
```

---

## 2) Create the **table** (if needed)
Target table name: **`CUSTOMERS`**  
Columns: **ID**, **FIRST_NAME**, **LAST_NAME**, **EMAIL**, **AGE**, **CITY**

```sql
CREATE OR REPLACE TABLE PUBLIC.CUSTOMERS (
  ID         INT,
  FIRST_NAME VARCHAR,
  LAST_NAME  VARCHAR,
  EMAIL      VARCHAR,
  AGE        INT,
  CITY       VARCHAR
);
```

> ‚ÑπÔ∏è If you already have `CUSTOMERS`, you can skip the `CREATE` or use `CREATE TABLE IF NOT EXISTS`.

---

## 3) Create a reusable **file format** for semicolon-separated CSV
We‚Äôll define a CSV file format that matches your file characteristics:
- **Delimiter**: **`;`**  
- **Header row**: **present (skip 1)**  
- **Quoted fields**: optional double quotes

```sql
CREATE OR REPLACE FILE FORMAT PUBLIC.FF_CSV_SEMICOLON
  TYPE = CSV
  FIELD_DELIMITER = ';'
  SKIP_HEADER = 1
  FIELD_OPTIONALLY_ENCLOSED_BY = '"'
  NULL_IF = ('', 'NULL');
```

---

## 4) Create an **external stage** pointing to S3
Stage name: **`STG_LOADINGDATA`**  
S3 URL: **`s3://snowflake-assignments-mc/loadingdata/`**

```sql
CREATE OR REPLACE STAGE PUBLIC.STG_LOADINGDATA
  URL = 's3://snowflake-assignments-mc/loadingdata/';
-- (No credentials here because we assume the bucket/prefix is public.
-- If not public, use a STORAGE INTEGRATION instead of embedding keys.)
```

> ‚úÖ **Best practice:** Prefer **STORAGE INTEGRATION** (IAM role) over embedding access keys.

---

## 5) **List** the files in the stage
```sql
LIST @PUBLIC.STG_LOADINGDATA;
```

You should see the data files you‚Äôre about to load.

---

## 6) **Load** the data into the **CUSTOMERS** table
We‚Äôll use explicit **column mapping** to guarantee the file-to-table order aligns:
**(ID, FIRST_NAME, LAST_NAME, EMAIL, AGE, CITY)**

```sql
COPY INTO PUBLIC.CUSTOMERS (ID, FIRST_NAME, LAST_NAME, EMAIL, AGE, CITY)
FROM @PUBLIC.STG_LOADINGDATA
FILE_FORMAT = (FORMAT_NAME = PUBLIC.FF_CSV_SEMICOLON)
ON_ERROR = 'ABORT_STATEMENT';  -- or 'CONTINUE' to skip bad rows
```

> üîé Use `ON_ERROR = 'CONTINUE'` if you want to keep good rows and review rejects later.

---

## 7) Verify **how many rows were loaded**
### A) From the **COPY** result (right after it runs)
The `COPY INTO` result set shows `rows_loaded` per file. To capture it in SQL:

```sql
-- Immediately after COPY, this scans the last query‚Äôs result:
SELECT SUM(TO_NUMBER(rows_loaded)) AS total_rows_loaded
FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));
```

### B) From the **table**
```sql
SELECT COUNT(*) AS total_rows_in_table
FROM PUBLIC.CUSTOMERS;
```

### C) From **copy history** (time window)
```sql
SELECT *
FROM TABLE(
  INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME => 'EXERCISE_DB.PUBLIC.CUSTOMERS',
    START_TIME => DATEADD('hour', -1, CURRENT_TIMESTAMP())
  )
)
ORDER BY START_TIME DESC;
```

---

## üß© Answers Required by the Assignment

- **How many rows have been loaded in this assignment?**  
  Run either of the verification queries in **Step 7**:
  - **From the COPY result:**  
    ```sql
    SELECT SUM(TO_NUMBER(rows_loaded)) AS total_rows_loaded
    FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()));
    ```
  - **From the table:**  
    ```sql
    SELECT COUNT(*) AS total_rows_in_table
    FROM PUBLIC.CUSTOMERS;
    ```

> The exact number depends on the files in the S3 path at the time you run the load.

---

## üõ†Ô∏è Troubleshooting Tips
- **`LIST` shows nothing** ‚Üí Verify the **URL** and that files exist; if bucket isn‚Äôt public, use a **storage integration**.
- **Access errors** ‚Üí Missing permissions; set up **STORAGE INTEGRATION** (IAM role with `s3:ListBucket` and `s3:GetObject`).
- **Bad parse / wrong columns** ‚Üí Confirm **delimiter `;`**, header row, and column order. If needed, load to a staging table first.
- **Mixed encodings** ‚Üí Ensure files are **UTF-8**.

---

## üìå Optional: Clean up
```sql
-- Drop stage (does NOT delete S3 files)
DROP STAGE IF EXISTS PUBLIC.STG_LOADINGDATA;

-- Drop file format
DROP FILE FORMAT IF EXISTS PUBLIC.FF_CSV_SEMICOLON;

-- Drop table (careful!)
-- DROP TABLE IF EXISTS PUBLIC.CUSTOMERS;

-- Drop database (careful!)
-- DROP DATABASE IF EXISTS EXERCISE_DB;
```

---
